---
title: "Práctica_2_IF_GCID"
author: "Nuria Oviedo y Sofía Fontan"
date: "2024-11-28"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducción

### 1. Log-verosimilitud de \( p \)


La *función de verosimilitud* y la *log-verosimilitud* son herramientas fundamentales en estadística para evaluar qué tan bien un conjunto de parámetros (\(\theta\)) explica los datos observados (\(x\)).

### Función de Verosimilitud:

La *función de verosimilitud* (\(L(\theta|x)\)) mide la probabilidad de observar un conjunto de datos, dado un conjunto de parámetros específicos. Para \(n\) datos (\(x_1, x_2, \dots, x_n\)), la verosimilitud se calcula como el producto de las funciones de densidad (o de probabilidad) para cada dato individual:
$$
L(\theta|x) = f(x|\theta) = \prod_{i=1}^{n} f(x_i|\theta)
$$

donde \(f(x_i|\theta)\) es la función de densidad (o de probabilidad) de \(x_i\) dado \(\theta\).

### Log-Verosimilitud:
Debido a que la función de verosimilitud puede implicar productos de muchos términos, es más práctico trabajar con su logaritmo natural, conocido como la *log-verosimilitud*:

$$

\ell(\theta|x) = \log L(\theta|x) = \sum_{i=1}^{n} \log f(x_i|\theta)
$$



La función de verosimilitud para \( n \) observaciones Bernoulli independientes es:
$$
L(p) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}
$$

Al tomar el logaritmo natural de la función de verosimilitud, obtenemos:
$$
\ell(p) = \sum_{i=1}^n \left( x_i \log(p) + (1-x_i) \log(1-p) \right)
$$

Agrupando términos:
$$
\ell(p) = \left(\sum_{i=1}^n x_i \right) \log(p) + \left( n - \sum_{i=1}^n x_i \right) \log(1-p)
$$

Definiendo \( r = \sum_{i=1}^n x_i \), la log-verosimilitud se puede expresar como:
$$
\ell(p) = r \log\left(\frac{p}{1-p}\right) + n \log(1-p)
$$

Dado que \( r \) es la suma de \( n \) variables Bernoulli i.i.d., su distribución es:
$$
r \sim \text{Binomial}(n, p)
$$

### 2. Información de Fisher para \( p \)

La información de Fisher está definida como:
$$
I(p) = -\mathbb{E}\left[\frac{\partial^2}{\partial p^2} \ell(p)\right]
$$

Para la log-verosimilitud obtenida:
$$
\frac{\partial^2 \ell(p)}{\partial p^2} = -\frac{r}{p^2} - \frac{n-r}{(1-p)^2}
$$

El valor esperado de \( r \) es \( \mathbb{E}[r] = np \). Sustituyendo, obtenemos:
$$
\mathbb{E}\left[\frac{\partial^2 \ell(p)}{\partial p^2}\right] = -\frac{np}{p^2} - \frac{n(1-p)}{(1-p)^2}
$$

Por lo tanto, la información de Fisher es:
$$
I(p) = \frac{n}{p(1-p)}
$$

### 3. Expresiones de \( Z \) y \( V \)

En este ejercicio, vamos a calcular las expresiones de los estadísticos \( Z \) y \( V \) para tres diferentes parametrizaciones de \( \theta \) en un modelo Bernoulli. Estos estadísticos son fundamentales en los contrastes de hipótesis y tienen relación con la diferencia entre el parámetro de interés \( p \) y el valor de referencia \( p_0 \).

La cantidad \( V \) es la información de Fisher, que mide la precisión de la estimación del parámetro en una muestra.

Vamos a resolver cada caso paso a paso.

### 1. Parámetro \( \theta = \log\left( \frac{p(1 - p_0)}{p_0(1 - p)} \right) \)

Primero, consideramos la parametrización \( \theta = \log\left( \frac{p(1 - p_0)}{p_0(1 - p)} \right) \). 

#### Paso 1: Log-verosimilitud

Sabemos que la log-verosimilitud para una muestra de tamaño \(n\) es:

\[
\ell(p) = r \log(p) + n \log(1 - p)
\]

donde \( r = \sum_{i=1}^n x_i \) es la suma de las observaciones de la muestra.

#### Paso 2: Derivada de la log-verosimilitud respecto a \( p \)

La derivada de la log-verosimilitud con respecto a \( p \) es:

\[
\frac{\partial \ell(p)}{\partial p} = \frac{r}{p} - \frac{n}{1 - p}
\]

#### Paso 3: Derivada de \( \theta \) respecto a \( p \)

Ahora, calculamos la derivada de \( \theta = \log\left( \frac{p(1 - p_0)}{p_0(1 - p)} \right) \) con respecto a \( p \):

\[
\frac{\partial \theta}{\partial p} = \frac{1 - p_0}{p(1 - p)}
\]

#### Paso 4: Estadístico \( Z \)

El estadístico \( Z \) se obtiene usando la regla de la cadena para derivar la log-verosimilitud con respecto a \( \theta \):

\[
Z = \frac{\partial}{\partial \theta} \ell(\theta) = \frac{n - n p_0}{\sqrt{n p_0 (1 - p_0)}}
\]

#### Paso 5: Información de Fisher \( V \)

La información de Fisher \( V \) es la varianza de \( Z \), y en este caso resulta ser:

\[
V = n p_0 (1 - p_0)
\]

### 2. Parámetro \( \theta = p - p_0 \)

Ahora, consideramos la parametrización \( \theta = p - p_0 \).

#### Paso 1: Log-verosimilitud

De nuevo, la log-verosimilitud es la misma que en el caso anterior:

\[
\ell(p) = r \log(p) + n \log(1 - p)
\]

#### Paso 2: Derivada de la log-verosimilitud respecto a \( p \)

La derivada respecto a \( p \) sigue siendo:

\[
\frac{\partial \ell(p)}{\partial p} = \frac{r}{p} - \frac{n}{1 - p}
\]

#### Paso 3: Derivada de \( \theta \) respecto a \( p \)

Para esta parametrización, la derivada de \( \theta = p - p_0 \) con respecto a \( p \) es simplemente:

\[
\frac{\partial \theta}{\partial p} = 1
\]

#### Paso 4: Estadístico \( Z \)

Dado que \( \frac{\partial \theta}{\partial p} = 1 \), el estadístico \( Z \) es:

\[
Z = \frac{r - n p_0}{\sqrt{\frac{n}{p_0(1 - p_0)}}}
\]

#### Paso 5: Información de Fisher \( V \)

La información de Fisher \( V \) es:

\[
V = \frac{n}{p_0 (1 - p_0)}
\]

### 3. Parámetro \( \theta = \arcsin \sqrt{p} - \arcsin \sqrt{p_0} \)

Finalmente, consideramos la parametrización \( \theta = \arcsin \sqrt{p} - \arcsin \sqrt{p_0} \).

#### Paso 1: Log-verosimilitud

La log-verosimilitud para una muestra de tamaño \( n \) es:

\[
\ell(p) = r \log(p) + n \log(1 - p)
\]

#### Paso 2: Derivada de la log-verosimilitud respecto a \( p \)

La derivada de la log-verosimilitud respecto a \( p \) sigue siendo la misma:

\[
\frac{\partial \ell(p)}{\partial p} = \frac{r}{p} - \frac{n}{1 - p}
\]

#### Paso 3: Derivada de \( \theta \) respecto a \( p \)

Para esta parametrización, la derivada de \( \theta = \arcsin \sqrt{p} - \arcsin \sqrt{p_0} \) con respecto a \( p \) es:

\[
\frac{\partial \theta}{\partial p} = \frac{1}{2 \sqrt{p(1 - p)}}
\]

#### Paso 4: Estadístico \( Z \)

El estadístico \( Z \) es:

\[
Z = \frac{2(r - n p_0)}{\sqrt{n p_0 (1 - p_0)}}
\]

#### Paso 5: Información de Fisher \( V \)

La información de Fisher \( V \) para esta parametrización es:

\[
V = 4n
\]

### Resumen

Las expresiones obtenidas para los estadísticos \( Z \) y \( V \) para cada parametrización de \( \theta \) son las siguientes:

1. Para \( \theta = \log\left( \frac{p(1 - p_0)}{p_0(1 - p)} \right) \):
   \[
   Z = \frac{n - n p_0}{\sqrt{n p_0 (1 - p_0)}}, \quad V = n p_0 (1 - p_0)
   \]

2. Para \( \theta = p - p_0 \):
   \[
   Z = \frac{r - n p_0}{\sqrt{\frac{n}{p_0 (1 - p_0)}}}, \quad V = \frac{n}{p_0 (1 - p_0)}
   \]

3. Para \( \theta = \arcsin \sqrt{p} - \arcsin \sqrt{p_0} \):
   \[
   Z = \frac{2(r - n p_0)}{\sqrt{n p_0 (1 - p_0)}}, \quad V = 4n
   \]


## Contraste de hipótesis

### 4.

```{r}
# Cálculo de Z y V para diferentes parametrizaciones

# Función para calcular Z y V para cada parametrización
calcular_Z_V <- function(r, n, p0, tipo_parametrizacion) {
  # Caso 1: Parámetro θ = log(p(1 - p0) / (p0(1 - p)))
  if (tipo_parametrizacion == "log_ratio") {
    Z <- (n - n * p0) / sqrt(n * p0 * (1 - p0))
    V <- n * p0 * (1 - p0)
  }
  
  # Caso 2: Parámetro θ = p - p0
  else if (tipo_parametrizacion == "diferencia") {
    Z <- (r - n * p0) / sqrt(n / (p0 * (1 - p0)))
    V <- n / (p0 * (1 - p0))
  }
  
  # Caso 3: Parámetro θ = arcsin(sqrt(p)) - arcsin(sqrt(p0))
  else if (tipo_parametrizacion == "arcsin") {
    Z <- (2 * (r - n * p0)) / sqrt(n * p0 * (1 - p0))
    V <- 4 * n
  }
  
  # Retornar los resultados
  return(list(Z = Z, V = V))
}

# Definir parámetros de la muestra
n <- 1000  # Tamaño de la muestra
p0 <- 0.3  # Proporción bajo la hipótesis nula
r <- 310   # Número de éxitos (por ejemplo, si 310 personas dijeron que prefieren estudiar de noche)

# Calcular Z y V para cada parametrización
resultados_log_ratio <- calcular_Z_V(r, n, p0, "log_ratio")
resultados_diferencia <- calcular_Z_V(r, n, p0, "diferencia")
resultados_arcsin <- calcular_Z_V(r, n, p0, "arcsin")

# Mostrar resultados
cat("Resultados para θ = log(p(1 - p0) / (p0(1 - p))):\n")
cat("Z:", resultados_log_ratio$Z, "\n")
cat("V:", resultados_log_ratio$V, "\n\n")

cat("Resultados para θ = p - p0:\n")
cat("Z:", resultados_diferencia$Z, "\n")
cat("V:", resultados_diferencia$V, "\n\n")

cat("Resultados para θ = arcsin(sqrt(p)) - arcsin(sqrt(p0)):\n")
cat("Z:", resultados_arcsin$Z, "\n")
cat("V:", resultados_arcsin$V, "\n")
```

### 5.

El cálculo de \( V \) y \( Z \) confirma que, para muestras suficientemente grandes (\( n \)), la distribución de \( Z \) tiende a ser aproximadamente Normal con media \( \theta \times V \) y varianza \( V \). Estos resultados son útiles para construir inferencias estadísticas, ya que permiten simplificar la complejidad del modelo original y trabajar con aproximaciones basadas en la distribución Normal.


```{r}
# Configuración inicial
set.seed(123) # Para reproducibilidad

# Parámetros del problema
n <- 1000  # Tamaño muestral
p0 <- 0.3  # Proporción bajo H0
p_observado <- 0.35  # Proporción observada en la muestra (ejemplo)
alpha <- 0.05  # Nivel de significancia

# Simulación de la muestra binomial
muestra <- rbinom(1, size = n, prob = p0) / n
cat("Proporción simulada bajo H0:", muestra, "\n")

# Estadístico Z
Z <- (p_observado - p0) / sqrt(p0 * (1 - p0) / n)
cat("Valor del estadístico Z:", Z, "\n")

# Cálculo del p-valor
p_valor <- 1 - pnorm(Z)
cat("P-valor:", p_valor, "\n")

# Decisión del contraste
if (p_valor < alpha) {
  cat("Rechazamos H0: Hay evidencia suficiente para aceptar H1.\n")
} else {
  cat("No rechazamos H0: No hay evidencia suficiente para aceptar H1.\n")
}


```
### Conclusión del ejercicio 5

**Resultados clave:**

1. **Proporción simulada bajo \( H_0 \):**  
   La proporción obtenida a partir de la simulación bajo la hipótesis nula \( H_0: p = 0.3 \) fue **0.29**, muy cercana al valor esperado (\( 0.3 \)). Esto indica que la simulación es consistente con la distribución definida bajo \( H_0 \).

2. **Estadístico \( Z \):**  
   El valor del estadístico fue **\( Z = 3.4503 \)**. Este mide cuán lejos está la proporción observada (\( p_{observado} = 0.35 \)) de la proporción propuesta bajo \( H_0 \). El valor alto de \( Z \) sugiere una diferencia significativa.

3. **P-valor:**  
   El \( p \)-valor calculado es **0.00028**, que es mucho menor que el nivel de significancia usual \( \alpha = 0.05 \). Esto implica que la probabilidad de observar una diferencia tan extrema o mayor, suponiendo que \( H_0 \) sea verdadera, es prácticamente nula.

4. **Decisión:**  
   Dado que el \( p \)-valor es menor que \( \alpha \), se **rechaza \( H_0 \)**. Por lo tanto, hay evidencia suficiente para aceptar \( H_1: p > 0.3 \).


## Ejercicio 6

## Cálculo del Tamaño Muestral para el Contraste de Hipótesis

En este análisis, queremos calcular el tamaño muestral \( n \) necesario para realizar un contraste de hipótesis bajo los siguientes supuestos:

- \( H_0: p = 0.3 \)
- \( H_1: p > 0.3 \)

Usamos el estadístico \( Z \) como herramienta de contraste, con un nivel de significancia \( \alpha \) y una potencia de \( 1 - \beta \). El procedimiento sigue los siguientes pasos:


#### Fórmula General

El tamaño muestral \( n \) se calcula a partir de la cantidad de información \( V \), que está relacionada directamente con \( n \). Para un contraste unilateral, la fórmula para \( V \) es:

\[
V = (((\frac{z_{\alpha}//2) + z_{\beta}}{\theta_R)})^2,
\]

donde:
- \( z_{\alpha} \): Percentil de una distribución normal estándar correspondiente al nivel de significancia \( \alpha \).
- \( z_{\beta} \): Percentil correspondiente a la potencia \(100( 1 - \beta \)).
- \( \theta_R \): Diferencia mínima que se desea detectar entre \( p \) y \( p_0 \) (es decir, \( \theta_R = p_{observado} - p_0 \)).

Reorganizando para \( n \), dado que \( V = \frac{1}{n} \), obtenemos:

\[
n = \frac{(z_{\alpha} + z_{\beta})^2}{\theta_R^2}.
\]

% Parametrización 1
n = \frac{\left(z_{\alpha/2} + z_{\beta}\right)^2}{\log^2\left(\frac{p(1-p_0)}{p_0(1-p)}\right)}

% Parametrización 2
n = \frac{\left(z_{\alpha/2} + z_{\beta}\right)^2}{(p - p_0)^2}

% Parametrización 3
n = \frac{\left(z_{\alpha/2} + z_{\beta}\right)^2}{\left(\arcsin\sqrt{p} - \arcsin\sqrt{p_0}\right)^2}



### 7. Estimación del tamaño muestral necesario

Queremos calcular el tamaño de muestra \( n \) necesario para detectar una diferencia entre \( p_0 = 0.003 \) y \( p = 0.006 \), con un nivel de significancia \( \alpha = 0.025 \) y una potencia \( 1 - \beta = 0.80 \).

Usamos la fórmula general del tamaño muestral para contrastes unilaterales:

\[
n = \frac{(z_{\alpha} + z_{\beta})^2}{\theta_R^2},
\]

donde:
- \( z_{\alpha} \): Percentil de la normal estándar para \( \alpha = 0.025 \),
- \( z_{\beta} \): Percentil para \( 1 - \beta = 0.80 \),
- \( \theta_R = p - p_0 = 0.006 - 0.003 = 0.003 \).

#### Código en R para el cálculo

```{r}

# Función para calcular el tamaño muestral en las tres parametrizaciones
sample_size <- function(p, p0, alpha, beta) {
  # Percentiles z_alpha/2 y z_beta
  z_alpha <- qnorm(1 - alpha / 2)
  z_beta <- qnorm(1 - beta)
  
  # Parametrización 1
  param1_n <- function(p, p0, z_alpha, z_beta) {
    theta1 <- log((p * (1 - p0)) / (p0 * (1 - p0)))
    numerador1 <- (z_alpha + z_beta)^2
    denominador1 <-theta1^2 * p0*(1-p0)
    n <- numerador1 / denominador1
    return(n)
  }
  
  # Parametrización 2
  param2_n <- function(p, p0, z_alpha, z_beta) {
    theta2 <- p - p0
    numerador2 <- (z_alpha + z_beta)^2 * (p0 * (1-p0))
    denominador2 <- theta2^2
    n <- numerador2 / denominador2
    return(n)
  }
  
  # Parametrización 3
  param3_n <- function(p, p0, z_alpha, z_beta) {
    theta3 <- asin(sqrt(p)) - asin(sqrt(p0))
    numerador3 <- (z_alpha + z_beta)^2
    denominador3 <- theta3^2*4
    n <- numerador3 / denominador3
    return(n)
  }
  
  # Calcular tamaños muestrales
  n1 <- param1_n(p, p0, z_alpha, z_beta)
  n2 <- param2_n(p, p0, z_alpha, z_beta)
  n3 <- param3_n(p, p0, z_alpha, z_beta)
  
  return(list(
    Parametrization_1 = n1,
    Parametrization_2 = n2,
    Parametrization_3 = n3
  ))
}

# Valores específicos
p <- 0.006       
p0 <- 0.003      
alpha <- 0.025  
beta <- 0.2    

# Cálculo del tamaño muestral
result <- sample_size(p, p0, alpha, beta)

# Mostrar resultados
print(result)


```
### 8. Estimación de errores tipo I y potencia del contraste

Queremos estimar:

1. **Error tipo I (\( \alpha \))**:
   - El error tipo I es la probabilidad de rechazar la hipótesis nula (\( H_0 \)) cuando esta es verdadera. 
   - En este caso, el nivel de significancia está fijado en \( \alpha = 0.025 \), por lo que este valor es constante para todas las parametrizaciones.

2. **Potencia (\( 1 - \beta \))**:
   - La potencia es la probabilidad de rechazar \( H_0 \) cuando la hipótesis alternativa (\( H_1 \)) es verdadera. 
   - Para calcular la potencia, necesitamos evaluar el estadístico \( Z \) bajo la distribución de \( H_1 \).

El estadístico \( Z \) se calcula como:

\[
Z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0 (1 - p_0)}{n}}},
\]

donde:
- \( \hat{p} \) es la proporción muestral bajo \( H_1 \) (en este caso \( \hat{p} = p \)),
- \( p_0 = 0.003 \) es la proporción bajo \( H_0 \),
- \( n \) es el tamaño muestral calculado para cada parametrización.

La potencia se calcula como la probabilidad de que \( Z > Z_\alpha \) bajo \( H_1 \):

\[
\text{Potencia} = P(Z > Z_\alpha \mid H_1),
\]

donde:
- \( Z_\alpha \) es el valor crítico correspondiente al nivel de significancia \( \alpha \) bajo la distribución normal estándar.

Vamos a calcular:
1. El error tipo I (\( \alpha \)) para cada parametrización, que se espera sea igual a 0.025.
2. La potencia (\( 1 - \beta \)) para los tamaños muestrales calculados anteriormente (\( n_1, n_2, n_3 \)) en las tres parametrizaciones.


```{r}
# Función para estimar error tipo I y potencia
calculate_error_and_power <- function(n, p0, p, alpha) {
  # Calcular Z_alpha (valor crítico bajo H0)
  Z_alpha <- qnorm(1 - alpha / 2)
  
  # Error tipo I (ya conocido)
  error_tipo_I <- alpha
  
  # Calcular estadístico Z bajo H1
  Z_H1 <- (p - p0) / sqrt((p0 * (1 - p0)) / n)
  
  # Calcular potencia (1 - beta)
  potencia <- 1 - pnorm(Z_alpha, mean = Z_H1, sd = 1)
  
  return(list(
    Error_Tipo_I = error_tipo_I,
    Potencia = potencia
  ))
}

# Tamaños muestrales calculados previamente
n1 <- 6614.339  # Parametrización 1
n2 <- 3158.841  # Parametrización 2
n3 <- 4596.236  # Parametrización 3

# Valores conocidos
p0 <- 0.003  # Proporción bajo H0
p <- 0.006   # Proporción bajo H1
alpha <- 0.025

# Calcular errores y potencia para cada parametrización
result_param1 <- calculate_error_and_power(n1, p0, p, alpha)
result_param2 <- calculate_error_and_power(n2, p0, p, alpha)
result_param3 <- calculate_error_and_power(n3, p0, p, alpha)

# Mostrar resultados
cat("Resultados para la Parametrización 1:\n")
print(result_param1)

cat("\nResultados para la Parametrización 2:\n")
print(result_param2)

cat("\nResultados para la Parametrización 3:\n")
print(result_param3)

```


### 9. Contrastes adicionales: Chi-cuadrado y prueba exacta

Además de calcular el tamaño muestral y los errores asociados, se aplicarán dos contrastes estadísticos adicionales para validar los resultados:

1. **Test de chi-cuadrado sin corrección de continuidad:**  
   Este test compara las frecuencias observadas y esperadas bajo \( H_0 \). Al no usar corrección de continuidad, el test es más sensible en muestras grandes. Se implementará con la función `chisq.test`, especificando que no se aplique corrección.

```{r}
# Valores de la muestra simulada
n <- ceiling(n)         # Tamaño muestral redondeado
r <- round(n * p)       # Casos observados de éxito

# Test de chi-cuadrado sin corrección de continuidad
chisq_result <- chisq.test(x = c(r, n - r), p = c(p0, 1 - p0), correct = FALSE)
cat("Chi-cuadrado sin corrección:\n")
print(chisq_result)

```



2. **Prueba exacta binomial:**  
   La prueba exacta binomial evalúa directamente la probabilidad de obtener el número de éxitos observados o más, bajo \( H_0 \). Esta prueba es especialmente útil para tamaños muestrales pequeños y se implementa con la función `binom.test`.
   
```{r}
# Prueba exacta binomial
binom_result <- binom.test(r, n, p0, alternative = "greater")
cat("Prueba exacta binomial:\n")
print(binom_result)

```


### 10.

Las **pruebas paramétricas** son aquellas que se basan en suposiciones sobre la distribución de los datos, como la normalidad o la homogeneidad de la varianza. 

Sin embargo, en ciertas situaciones, estas suposiciones no se cumplen, lo que invalida la aplicación de pruebas paramétricas. Razones por las que las pruebas paramétricas pueden no ser adecuadas:

#### Razones para Utilizar Pruebas No Paramétricas

1. **Tamaño de muestra pequeño**:
   Las pruebas paramétricas requieren un tamaño de muestra suficientemente grande para que las estimaciones sean precisas y la distribución muestral se aproxime a la normalidad. Con muestras pequeñas, las distribuciones pueden no ser normales, lo que invalida las pruebas paramétricas.

2. **Distribución no normal**:
   Las pruebas paramétricas asumen que los datos siguen una distribución normal. Si los datos no siguen una distribución normal (son sesgados o tienen colas pesadas), las pruebas paramétricas pueden producir resultados erróneos.

3. **Escalas de medición no adecuadas**:
   Las pruebas paramétricas requieren que los datos sean al menos de intervalo, es decir, que se pueda medir la distancia entre valores. Si los datos son de una escala nominal o ordinal, las pruebas paramétricas no son apropiadas.

4. **Heterogeneidad en la varianza**:
   Muchas pruebas paramétricas asumen homogeneidad de varianza (es decir, que las varianzas en los grupos son iguales). Si no se cumple, los resultados de las pruebas pueden no ser confiables.

Cuando las suposiciones necesarias para las pruebas paramétricas no se cumplen, las **pruebas no paramétricas** son una opción adecuada. Estas pruebas no requieren suposiciones sobre la distribución de los datos, lo que las hace más flexibles y aplicables a una mayor variedad de situaciones.

#### Pruebas No Paramétricas

Son más flexibles y menos sensibles a valores atípicos y a la violación de los supuestos de normalidad.

Algunas pruebas no paramétricas son:
- **Prueba exacta binomial**: Utilizada para comparar una proporción observada con un valor de referencia cuando los datos son binomiales.
- **Prueba de Kolmogorov-Smirnov**: Para comparar dos distribuciones.
- **Prueba de Mann-Whitney**: Para comparar dos muestras independientes.
- **Prueba de Kruskal-Wallis**: Para comparar más de dos muestras independientes.

#### ¿Qué prueba no paramétrica utilizar para comparar una proporción con un valor de referencia?

Para comparar una proporción observada en una muestra con un valor de referencia \( p_0 \) cuando no se cumplen los supuestos de pruebas paramétricas, utilizamos la **prueba exacta binomial**. Esta prueba es adecuada cuando tenemos datos binomiales (con dos posibles resultados, como éxito o fracaso) y queremos comparar la proporción de éxitos observados con una proporción teórica \( p_0 \).

La hipótesis nula en esta prueba es que la proporción observada es igual a la proporción de referencia \( p_0 \), y la hipótesis alternativa puede ser que la proporción observada es mayor o menor que \( p_0 \).

---

#### Prueba Exacta Binomial en R

- `x`: Es el número de éxitos observados en la muestra.
- `n`: Es el tamaño de la muestra.
- `p`: Es la proporción bajo la hipótesis nula \( p_0 \).
- `alternative`: Define la hipótesis alternativa. Puede ser `"greater"` para probar si la proporción observada es mayor que \( p_0 \), `"less"` para probar si es menor, o `"two.sided"` para probar si es diferente.

#### Ejemplo en R

Supongamos que tenemos una muestra de 50 observaciones en la que 12 son éxitos. Queremos comparar la proporción observada con una proporción de referencia \( p_0 = 0.3 \). El código en R sería el siguiente:

```{r}
# Parámetros del problema
n <- 50             # Tamaño de la muestra
x <- 12             # Número de éxitos observados
p0 <- 0.3           # Proporción bajo H0 (valor de referencia)
alpha <- 0.05       # Nivel de significancia

# Realizar la prueba exacta binomial
resultado <- binom.test(x = x, n = n, p = p0, alternative = "greater")

# Mostrar los resultados principales de la prueba
cat("Resultados de la prueba exacta binomial:\n")
print(resultado)

# Interpretar los resultados basados en el p-valor
if (resultado$p.value < alpha) {
  cat("\nConclusión: Se rechaza H0. La proporción observada es significativamente mayor que", p0, ".\n")
} else {
  cat("\nConclusión: No se rechaza H0. No hay evidencia suficiente para concluir que la proporción es mayor que", p0, ".\n")
}
```



